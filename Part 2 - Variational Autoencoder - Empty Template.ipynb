{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59f8860-2eba-41b6-ab69-93660c8aa994",
   "metadata": {},
   "source": [
    "# Introduction to Variational Autoencoders with Pyro\n",
    "Welcome to this introductory Jupyter Notebook on Variational Autoencoders (VAEs) built using Pyro! In this tutorial, we will explore the fascinating world of generative models and dive into the inner workings of VAEs.\n",
    "\n",
    "Generative models are a powerful class of machine learning algorithms that can learn to generate new data samples similar to the training data. VAEs, in particular, are a type of generative model that combines techniques from both deep learning and probabilistic modeling. They allow us to capture complex patterns and generate new samples from high-dimensional data distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16668490-0237-434e-ab01-308b794a4319",
   "metadata": {},
   "source": [
    "# Sections\n",
    "0. Setup\n",
    "1. Defining the Encoder\n",
    "2. Defining the Decoder\n",
    "3. Defining the VAE (Model & Guide)\n",
    "4. Inference and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507dee40-c7eb-4e31-8295-8c5c23b2fe27",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "In this section, we will set up the necessary environment and dependencies for working with Variational Autoencoders (VAEs) in Pyro. We will install any required packages, import the necessary libraries, and load the dataset (if applicable). It is important to have a properly configured environment before proceeding to the subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e4c85-2608-4d93-8e04-9a98bf2f9228",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt\n",
    "\n",
    "import pyro\n",
    "import torch\n",
    "from pyro import distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample, PyroParam\n",
    "from pyro.contrib.examples.util import MNIST\n",
    "import pyro.poutine as poutine\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "from torchsummary import summary\n",
    "from itertools import chain\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfeb7ec-72d5-46df-bcf3-b9b72d27ec35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the datatloaders for training and testing data\n",
    "def setup_data_loaders(batch_size=128):\n",
    "    root = './data'\n",
    "    download = True\n",
    "    trans = transforms.ToTensor()\n",
    "    train_set = MNIST(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        transform=trans,\n",
    "        download=download\n",
    "    )\n",
    "    test_set = MNIST(\n",
    "        root=root,\n",
    "        train=False,\n",
    "        transform=trans,\n",
    "        download=download\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba462d8-fc8e-49ca-9aab-0370105cf966",
   "metadata": {},
   "source": [
    "## 1. Defining the Encoder\n",
    "The encoder is a crucial component of the Variational Autoencoder (VAE) architecture. In this section, we will define the encoder network, which takes an input data sample and maps it to the corresponding latent space representation. We will explore various architectures and techniques for constructing the encoder network, such as fully connected layers, convolutional layers, and activation functions. Understanding the encoder's role and designing an effective architecture is essential for the overall performance of the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e506cadb-e0eb-456f-973c-efa8369462ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up layers\n",
    "        self.conv1 = None\n",
    "        self.conv2 = None\n",
    "        self.batchnorm = None\n",
    "        self.fc_mean = None\n",
    "        self.fc_std = None\n",
    "        \n",
    "        # Set up non-linearities\n",
    "        pass\n",
    "        \n",
    "    def forward(self, X):\n",
    "        z_loc = None\n",
    "        z_scale = None\n",
    "        \n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb26da3-afc5-43fe-9fc6-5030e2f38300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(Encoder(), (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1daf8-74cb-4c2d-bb67-84e1a95ce634",
   "metadata": {},
   "source": [
    "## 2. Defining the Decoder\n",
    "The decoder complements the encoder in the VAE framework. It takes a latent space representation and reconstructs the original input data sample. In this section, we will define the decoder network, which is responsible for generating the output based on the latent variables. We will discuss different architectural choices for the decoder, including deconvolutional layers, transposed convolutional layers, and non-linear activation functions. A well-designed decoder is crucial for generating high-quality samples from the learned latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda4311-a33f-4df4-939b-a2c2b1c45ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up layers\n",
    "        self.fc_mean_std = None\n",
    "        self.deconv2 = None\n",
    "        self.deconv1 = None\n",
    "        self.fc_out = None\n",
    "        \n",
    "        # Set up non-linearities and transformations\n",
    "        pass\n",
    "\n",
    "        \n",
    "    def forward(self, z):\n",
    "        loc_out = None\n",
    "        \n",
    "        return loc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b0579-b1db-4af3-9637-6d1eb68dff51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(Decoder(), (32,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b6152-6782-4636-ae64-b37207b0f30d",
   "metadata": {},
   "source": [
    "## 3. Defining the VAE (Model & Guide)\n",
    "In this section, we will bring together the encoder and decoder components to define the complete Variational Autoencoder (VAE) model using Pyro. We will specify the probabilistic model and the guide, which will be used for posterior inference. We will define the priors, likelihood functions, and latent variables, as well as discuss the necessary modifications to the standard VAE formulation. Understanding the model and guide definitions is essential for training and inference in VAEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff987a52-1941-44de-8e1e-2e831f31d7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels: int = 1, hidden_channels: int = 32, latent_channels: int = 1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "        self.z_dim = 32\n",
    "        \n",
    "    def model(self, X=None, samples_to_generate=None):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with None:\n",
    "            z_loc = None\n",
    "            z_scale = None\n",
    "            z = None\n",
    "\n",
    "            loc_out = None\n",
    "            pyro.sample(None)\n",
    "            \n",
    "            return loc_out\n",
    "            \n",
    "    def guide(self, X, samples_to_generate=None):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "\n",
    "        with None:\n",
    "            z_loc, z_scale = None, None\n",
    "            pyro.sample(None)\n",
    "            \n",
    "    def reconstruct_img(self, x):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3dbbd3-2e95-4a5d-afe7-fc18ad62103e",
   "metadata": {},
   "source": [
    "## 4. Training the VAE\n",
    "\n",
    "In this section, we will dive into the training process of our Variational Autoencoder (VAE). Now that we have defined the VAE model and guide, it's time to optimize its parameters and make the model learn from our data. Training a VAE involves maximizing the evidence lower bound (ELBO) objective, which balances the reconstruction loss and the regularization term imposed by the variational distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2fe15-750f-4e45-bae5-8065f29b8987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae = None\n",
    "optimizer = None\n",
    "svi = pyro.infer.SVI(\n",
    "    model=None,\n",
    "    guide=None,\n",
    "    optim=None,\n",
    "    loss=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e5375-fb90-48fe-8b08-b2d2013e8d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = None\n",
    "\n",
    "train_loader, test_loader = setup_data_loaders()\n",
    "\n",
    "loss = []\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), total=EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for batch, _ in train_loader:\n",
    "        pass\n",
    "        \n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    loss.append(total_epoch_loss_train)\n",
    "    if (epoch % 10) == 0:\n",
    "        print(total_epoch_loss_train)\n",
    "\n",
    "px.line(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f2577-b0f2-44bb-85e4-f9f11b6be3ba",
   "metadata": {},
   "source": [
    "## 5. Generating Images with the Trained VAE\n",
    "\n",
    "In this section, we will explore the capabilities of our trained Variational Autoencoder (VAE) by generating new images. By sampling from the learned latent space, we can generate novel data samples that capture the underlying patterns and variations present in the training dataset. We will examine the test loss, visualize the test images in a T-SNE projection of the latent space, and finally generate new images by sampling from the latent space distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514058f-af39-47aa-ae52-1b4ec2dd6e02",
   "metadata": {},
   "source": [
    "### 5.1 Test Loss\n",
    "\n",
    "In this sub-section, we evaluate the performance of our trained VAE by calculating the test loss across all the testing samples. The test loss provides an indication of how well our VAE generalizes to unseen data. By comparing the test loss to the training loss, we can assess if the model exhibits overfitting or underfitting tendencies. A lower test loss suggests that the VAE has learned meaningful representations and can reconstruct the testing images effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73cf4ed-d7ce-4651-8368-0ec3f713926b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(svi, test_loader):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for x, _ in test_loader:\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ec9cc-f384-49bf-bc03-03c243895d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate(svi=svi, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a8e3c-b734-42fd-b069-6626fbf498eb",
   "metadata": {},
   "source": [
    "### 5.2 Test Images in T-SNE\n",
    "\n",
    "In this sub-section, we visualize the test images in a T-SNE projection of the latent space. T-SNE (t-Distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique commonly used for visualizing high-dimensional data in a lower-dimensional space. By projecting the latent representations of the test images onto a 2D or 3D space, we can gain insights into the clustering and structure of the latent space. This visualization allows us to understand how well the VAE has captured the underlying data distribution and if similar images are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d5100-74c7-419e-9f94-aba38a1ef4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put test images in latent\n",
    "\n",
    "latent_images = [\n",
    "    vae.encoder(img)[0].detach().cpu().numpy() for img, _ in test_loader\n",
    "]\n",
    "labels = [label.detach().cpu().numpy().astype(str) for _, label in test_loader]\n",
    "labels = list(chain.from_iterable(labels))\n",
    "latent_images = np.concatenate(latent_images, axis=0)\n",
    "\n",
    "tsne = TSNE(n_components=2, n_jobs=-1, random_state=0, metric=\"cosine\", verbose=10)\n",
    "latent_embedding = tsne.fit_transform(latent_images)\n",
    "px.scatter(x=latent_embedding[:, 0], y=latent_embedding[:, 1], color=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e9fc8-5215-4886-80c5-e5d84e0b3470",
   "metadata": {},
   "source": [
    "### 5.3 New Images\n",
    "\n",
    "In this sub-section, we harness the power of the trained VAE to generate new images. By sampling from the learned latent space distribution, we can explore the generative capabilities of the model. We will randomly sample latent vectors from the prior distribution or systematically traverse the latent space to observe the variations in the generated images. This provides an exciting opportunity to create novel, never-before-seen images that resemble the patterns learned during training. We can adjust the latent variables to influence the generated images and explore the continuous variations in the generated samples.\n",
    "\n",
    "By the end of this section, you will have a comprehensive understanding of the VAE's performance on the test dataset, visual insights into the latent space using T-SNE, and the ability to generate new images. Let's proceed and unlock the generative power of our trained VAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c359084-ae93-4fe3-8465-9c949cd6bebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise = torch.zeros([28, 28])\n",
    "sample_loc = vae.model(noise).reshape(-1, 1, 28, 28)\n",
    "img = vae.reconstruct_img(sample_loc).detach().cpu().numpy().reshape(-1, 28, 28)\n",
    "\n",
    "def gen(imgs):\n",
    "    for img in imgs:\n",
    "        yield img\n",
    "        \n",
    "img = gen(imgs=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6223505-60e9-402a-b05d-dfa73e727525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(next(img) * 255)).resize((100, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
